"""
Alternativa para IA local usando Ollama ou modelos open-source
Use este arquivo se n√£o quiser usar OpenAI
"""

import requests
import json
from datetime import datetime

class LocalAI:
    def __init__(self, model_url="http://localhost:11434"):
        self.model_url = model_url
        self.model_name = "llama2"  # ou outro modelo dispon√≠vel
    
    def generate_response(self, user_question, context_data=None, schema=None):
        """Gera resposta usando IA local (Ollama)"""
        try:
            system_prompt = f"""
            Voc√™ √© um assistente especializado em an√°lise de dados de um sistema ERP empresarial.
            
            Esquema do banco de dados dispon√≠vel:
            {schema}
            
            Instru√ß√µes:
            1. Responda perguntas sobre os dados do sistema em portugu√™s
            2. Se precisar de dados espec√≠ficos, sugira uma consulta SQL segura (apenas SELECT)
            3. Seja claro, objetivo e profissional
            4. Foque em insights √∫teis para o neg√≥cio
            5. Se n√£o souber algo, seja honesto
            
            Dados de contexto: {context_data}
            
            Pergunta do usu√°rio: {user_question}
            """
            
            # Requisi√ß√£o para Ollama
            response = requests.post(
                f"{self.model_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": system_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 500
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'Erro ao gerar resposta')
            else:
                return "Erro: Servi√ßo de IA local n√£o dispon√≠vel"
                
        except requests.exceptions.RequestException:
            # Fallback para respostas pr√©-definidas
            return self.fallback_response(user_question)
        except Exception as e:
            return f"Erro ao processar pergunta: {str(e)}"
    
    def fallback_response(self, user_question):
        """Respostas de fallback quando a IA n√£o est√° dispon√≠vel"""
        question_lower = user_question.lower()
        
        responses = {
            'faturamento': "üìä Para consultar o faturamento, acesse o m√≥dulo Financeiro ‚Üí Relat√≥rios ‚Üí Faturamento Mensal",
            'fornecedor': "üìã Informa√ß√µes sobre fornecedores est√£o dispon√≠veis em Cadastros ‚Üí Fornecedores",
            'estoque': "üì¶ Para verificar o estoque, acesse Estoque ‚Üí Consulta de Produtos",
            'vendas': "üí∞ Relat√≥rios de vendas est√£o em Vendas ‚Üí Relat√≥rios ‚Üí Vendas por Per√≠odo",
            'cliente': "üë• Dados de clientes est√£o em Cadastros ‚Üí Clientes",
            'produto': "üè∑Ô∏è Informa√ß√µes de produtos em Cadastros ‚Üí Produtos"
        }
        
        for keyword, response in responses.items():
            if keyword in question_lower:
                return response
        
        return """
        ü§ñ Desculpe, n√£o consegui processar sua pergunta no momento. 
        
        Voc√™ pode tentar:
        ‚Ä¢ Perguntas sobre faturamento e vendas
        ‚Ä¢ Consultas sobre fornecedores e clientes
        ‚Ä¢ Informa√ß√µes sobre estoque e produtos
        ‚Ä¢ Relat√≥rios financeiros
        
        Ou navegue pelos menus do sistema para encontrar as informa√ß√µes desejadas.
        """

# Fun√ß√£o para usar no app.py
def generate_local_ai_response(user_question, context_data=None, schema=None):
    """Fun√ß√£o wrapper para usar no app principal"""
    ai = LocalAI()
    return ai.generate_response(user_question, context_data, schema)